{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9520883-fb9c-42fd-8d73-d19e3a401c7f",
   "metadata": {},
   "source": [
    "# Exercise: XGBoost\n",
    "\n",
    "In this exercise, we'll be exploring XGBoost and leveraging it on the same regression and classification problem as `Exercise: Linear Models`. This is useful as when are you faced with a new challenge, you often try many different models to see how they each perform on your problem space. XGBoost can be tuned in many different ways, so we'll stick with some of the simpler defaults.\n",
    "\n",
    "You're tasked with completing the following steps:\n",
    "1. Load in the wine dataset from scikit learn.\n",
    "2. For the wine dataset, create a train and test split, 80% train / 20% test.\n",
    "3. Load the train/test data into the xgboost matrix\n",
    "4. Create a XGBoost Classifier model with these hyper parameters:\n",
    "    1. max_depth: 5\n",
    "    2. eta: 0.1\n",
    "    3. objective: multi:softmax\n",
    "    4. num_class: 3\n",
    "    5. num_round: 100\n",
    "5. Evaluate the model with the test dataset\n",
    "6. Output the feature importance of the wine dataset\n",
    "7. Load the diabetes dataset from scikit learn\n",
    "8. For the Diabetes dataset, create a train and test split, 80% train / 20% test.\n",
    "9. Load the train/test data into the xgboost matrix\n",
    "10. Create a XGBoost Regression model model with these hyper parameters:\n",
    "    1. max_depth: 2\n",
    "    2. eta: 0.03\n",
    "    3. gamma: 0.09\n",
    "    4. colsample_bytree: 0.5\n",
    "    5. objective: reg:squarederror\n",
    "    6. num_round: 100\n",
    "11. Evaluate the model with the test dataset\n",
    "12. Output the feature importance of the diabetes dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a458f2-c56e-495d-a8c2-db7e8860e507",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faffb8fe-667a-490b-b512-2d9dce41e0e5",
   "metadata": {},
   "source": [
    "### Open up Sagemaker Studio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc61b6e7-780d-4aef-9287-2b9b9f68c182",
   "metadata": {},
   "source": [
    "1. Notebook should be using a `ml.t3.medium` instance (2 vCPU + 4 GiB)\n",
    "2. Notebook should be using kernal: `Python 3 (Data Science)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28b0b692-3c1d-4ad3-9535-7a42e38ec0f6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyring is skipped due to an exception: 'keyring.backends'\n",
      "Requirement already satisfied: xgboost in /opt/conda/lib/python3.7/site-packages (1.6.2)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from xgboost) (1.21.6)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from xgboost) (1.4.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# xgboost is not preinstalled so we'll need to install it manually\n",
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6dfefd6d-ed7f-46e3-a0c0-befda8d6aa23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.metrics import r2_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e6fd8c-df16-4109-a7a8-6c6ccb2cdf29",
   "metadata": {},
   "source": [
    "## XGBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc9c70a0-c311-4cf7-befb-92641f071326",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load in the wine dataset\n",
    "wine = datasets.load_wine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d674674-0c0f-4a2a-8d0a-c8b1fc4c11a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the wine `data` dataset as a dataframe and name the columns with `feature_names`\n",
    "df = pd.DataFrame(wine['data'], columns=wine['feature_names'] )\n",
    "\n",
    "# Include the target as well\n",
    "df['target'] = wine['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75ee9c82-4e05-4952-8be9-7f823436d32b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split your data with these ratios: train: 0.8 | test: 0.2\n",
    "df_train, df_test = train_test_split(df, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "185f38aa-56a6-470c-9238-640b11214bcd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load your train/test dataframe into DMatrix\n",
    "dtrain = xgb.DMatrix(df_train[wine['feature_names']], label=df_train['target'])\n",
    "dtest = xgb.DMatrix(df_test[wine['feature_names']], label=df_test['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7205ac08-20cd-48c5-a90e-44960f6d81f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# How does the model perform on the training dataset and default model parameters?\n",
    "# Using the hyperparameters in the requirements, is there improvement?\n",
    "# Remember we use the test dataset to score the model\n",
    "param = {'max_depth': 5, 'eta': 0.1, 'objective': 'multi:softmax', 'num_class': 3}\n",
    "num_round = 100\n",
    "bst = xgb.train(param, dtrain, num_round)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "373d5630-84b5-4fbb-9b9a-08871ec4fe98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# xgboost is not scikit learn, so you'll need to do predictions using their API\n",
    "preds = bst.predict(dtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7af1a25e-1c1a-49ce-a27b-eade3c5b215f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9444444444444444"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accuracy score using scikit learn function for classification metric\n",
    "accuracy_score(preds, df_test['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff215bae-6b1b-43cb-8d9e-957b5f50190f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f036025fb10>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Plot the importance of the features based on fitted trees\n",
    "xgb.plot_importance(bst)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18b9979-db51-44eb-a2ba-c5fc6fab843b",
   "metadata": {},
   "source": [
    "## XGBoost Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6fe39b07-53b3-4576-b392-1a1df77b43f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load in the diabetes dataset\n",
    "diabetes = datasets.load_diabetes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e36b9530-5bcd-40b4-a1f0-e6ad32bac145",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the diabetes `data` dataset as a dataframe and name the columns with `feature_names`\n",
    "dfd = pd.DataFrame(diabetes['data'], columns=diabetes['feature_names'] )\n",
    "\n",
    "# Include the target as well\n",
    "dfd['target'] = diabetes['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "85a9b895-00a5-48b9-951f-e36436255d5c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split your data with these ratios: train: 0.8 | test: 0.2\n",
    "dfd_train, dfd_test = train_test_split(dfd, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "11ca6d04-4ba7-4aba-834e-9e11afb5c22e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load your train/test dataframe into DMatrix\n",
    "dtrain = xgb.DMatrix(dfd_train[diabetes['feature_names']], label=dfd_train['target'])\n",
    "dtest = xgb.DMatrix(dfd_test[diabetes['feature_names']], label=dfd_test['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1b0f9d06-9436-4a4e-988d-f0ad00b26e10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# How does the model perform on the training dataset and default model parameters?\n",
    "# Using the hyperparameters in the requirements, is there improvement?\n",
    "# Remember we use the test dataset to score the model\n",
    "param = {'max_depth': 2, 'eta': 0.03, 'objective': 'reg:squarederror', 'colsample_bytree': 0.5, 'gamma': 0.09}\n",
    "num_rounds = 100\n",
    "bst = xgb.train(param, dtrain, num_rounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "98c39401-a8cb-4e6c-aabb-c318f384e423",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# xgboost is not scikit learn, so you'll need to do predictions using their API\n",
    "preds = bst.predict(dtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5802ec80-f1c1-45b8-85fe-43ec349fec01",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3214960872257303"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# R2 score using scikit learn function for regression metric\n",
    "r2_score(dfd_test['target'], preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8db9e601-7bd7-47ea-aa07-2f2c7b3649ff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f0359a66c90>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Plot the importance of the features based on fitted trees\n",
    "xgb.plot_importance(bst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e0fc1b-0feb-4c9a-9761-f4a9714a549c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
